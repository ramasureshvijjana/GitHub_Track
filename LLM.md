# 📘 LLM Textbook Index

A structured overview of NLP topics covered in below.
Follow the link for full practice code on all LLM topics: [LLM Code Repo]()

| 📝 **Topic Name**      | 📂 **Type**       | 📚 **Main Topic**    |
|------------------------|-------------------|-----------------------|
|[1. Transformer architecture](https://github.com/ramasureshvijjana/LLM/blob/master/01_Transformer_architecture.md)| &emsp;Topic | &emsp;✅ |
|[2. Pre-training and Fine-tuning]()| &emsp;Topic | &emsp;✅ |
|[3. Input Token Embedding Techniques]() <br>&emsp;[3.1 Word Embeddings]() <br>&emsp;[3.2 Positional Embeddings]() <br>&emsp;[3.3 Subword Embeddings]()| &emsp;Topic <br>&emsp;Topic <br>&emsp;Topic <br>&emsp;Topic | &emsp;✅ <br>&emsp;✅ <br>&emsp;✅ <br>&emsp;✅|
|[4. Dense vector]()| &emsp;Topic | &emsp;✅ |
|[5. LLM Learning Types]() <br>&emsp;[5.1 Autoencoding Learning]() <br>&emsp;[5.2 Autoregressive Learning]() <br>&emsp;[5.3 Masked Language Modeling]() <br>&emsp;[5.4 Sequence Learning]()| &emsp;Topic <br>&emsp;Topic <br>&emsp;Topic <br>&emsp;Topic <br>&emsp;Topic | &emsp;✅ <br>&emsp;✅ <br>&emsp;✅ <br>&emsp;✅ <br>&emsp;✅|
|[4. BERT Model]()| &emsp;Topic | &emsp;✅ |
|[4. GPT Model]()| &emsp;Topic | &emsp;✅ |
|[4. XLNET Model]()| &emsp;Topic | &emsp;✅ |

> ✨ *Tip: Use consistent naming and categorization to keep your index organized and searchable.*
