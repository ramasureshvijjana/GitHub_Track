# 📘 LLM Textbook Index

A structured overview of NLP topics covered in below.
Follow the link for full practice code on all LLM topics: [LLM Code Repo]()

| 📝 **Topic Name**      | 📂 **Type**       | 📚 **Main Topic**    |
|------------------------|-------------------|-----------------------|
|[1. Transformer architecture](https://github.com/ramasureshvijjana/LLM/blob/master/01_Transformer_architecture.md)| &emsp;Topic | &emsp;✅ |
|[2. Pre-training and Fine-tuning]() <br>&emsp;[2.1 LoRA]() <br>&emsp;[2.2 QLoRA]() <br>&emsp;[2.3 PEFT]() <br>&emsp;[2.4 Quantization]() <br>&emsp;[2.5 Distillation]()| &emsp;Topic <br>&emsp;Topic <br>&emsp;Topic <br>&emsp;Topic <br>&emsp;Topic <br>&emsp;Topic| &emsp;✅ <br>&emsp;✅ <br>&emsp;✅ <br>&emsp;✅ <br>&emsp;✅ <br>&emsp;✅ |
|[3. Input Token Embedding Techniques]() <br>&emsp;[3.1 Word Embeddings]() <br>&emsp;[3.2 Positional Embeddings]() <br>&emsp;[3.3 Subword Embeddings]()| &emsp;Topic <br>&emsp;Topic <br>&emsp;Topic <br>&emsp;Topic | &emsp;✅ <br>&emsp;✅ <br>&emsp;✅ <br>&emsp;✅|
|[4. Dense vector]()| &emsp;Topic | &emsp;✅ |
|[5. LLM Learning Types]() <br>&emsp;[5.1 Autoencoding Learning]() <br>&emsp;[5.2 Autoregressive Learning]() <br>&emsp;[5.3 Masked Language Modeling]() <br>&emsp;[5.4 Sequence Learning]()| &emsp;Topic <br>&emsp;Topic <br>&emsp;Topic <br>&emsp;Topic <br>&emsp;Topic | &emsp;✅ <br>&emsp;✅ <br>&emsp;✅ <br>&emsp;✅ <br>&emsp;✅|
|[6. BERT Model]()| &emsp;Topic | &emsp;✅ |
|[7. GPT Model]()| &emsp;Topic | &emsp;✅ |
|[8. XLNET Model]()| &emsp;Topic | &emsp;✅ |
|[9. Decoding Strategies]() <br>&emsp;[9.1 Greedy]() <br>&emsp;[9.2 Beam]() <br>&emsp;[9.3 Top-K]() <br>&emsp;[9.4 Top-P]() <br>&emsp;[9.5 Temperature]()| &emsp;Topic <br>&emsp;Topic <br>&emsp;Topic <br>&emsp;Topic <br>&emsp;Topic <br>&emsp;Topic| &emsp;✅ <br>&emsp;✅ <br>&emsp;✅ <br>&emsp;✅ <br>&emsp;✅ <br>&emsp;✅ |
|[10. Prompt Engineering]() <br>&emsp;[10.1 Zero-shot, One-shot, Few-shot and incontext learning]() <br>&emsp;[10.2 Designing Prompts]() <br>&emsp;[10.3 Prompt Templates]() <br>&emsp;[10.4 System vs User Prompts]() | &emsp;Topic <br>&emsp;Topic <br>&emsp;Topic <br>&emsp;Topic <br>&emsp;Topic| &emsp;✅ <br>&emsp;✅ <br>&emsp;✅ <br>&emsp;✅ <br>&emsp;✅ |
|[11. Dealing with LLM Problems]() <br>&emsp;[11.1 Hallucination]() <br>&emsp;[11.2 Mis-alignment]() <br>&emsp;[11.3 Bias]()| &emsp;Topic <br>&emsp;Topic <br>&emsp;Topic <br>&emsp;Topic | &emsp;✅ <br>&emsp;✅ <br>&emsp;✅ <br>&emsp;✅|
|[12. RAG]()| &emsp;Topic | &emsp;✅ |
|[13. Evalution matrics]() <br>&emsp;[13.1 BLEU]() <br>&emsp;[13.2 ROUGE]() <br>&emsp;[13.3 perplexity]() <br>&emsp;[13.4 human evalution]()| &emsp;Topic <br>&emsp;Topic <br>&emsp;Topic <br>&emsp;Topic <br>&emsp;Topic | &emsp;✅ <br>&emsp;✅ <br>&emsp;✅ <br>&emsp;✅<br>&emsp;✅|
|[14. Benchmarks]() <br>&emsp;[14.1 MMLU]() <br>&emsp;[14.2 HELM]() <br>&emsp;[14.3 Big-Bench]() <br>&emsp;[14.4 HumanEvaL]()| &emsp;Topic <br>&emsp;Topic <br>&emsp;Topic <br>&emsp;Topic <br>&emsp;Topic | &emsp;✅ <br>&emsp;✅ <br>&emsp;✅ <br>&emsp;✅ <br>&emsp;✅|
> ✨ *Tip: Use consistent naming and categorization to keep your index organized and searchable.*
