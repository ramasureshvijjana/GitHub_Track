# ğŸ“˜ LLM Textbook Index

A structured overview of NLP topics covered in below.
Follow the link for full practice code on all LLM topics: [LLM Code Repo]()

| ğŸ“ **Topic Name**      | ğŸ“‚ **Type**       | ğŸ“š **Main Topic**    |
|------------------------|-------------------|-----------------------|
|[1. Transformer architecture](https://github.com/ramasureshvijjana/LLM/blob/master/01_Transformer_architecture.md)| &emsp;Topic | &emsp;âœ… |
|[2. Pre-training and Fine-tuning]()| &emsp;Topic | &emsp;âœ… |
|[3. Input Token Embedding Techniques]() <br>&emsp;[3.1 Word Embeddings]() <br>&emsp;[3.2 Positional Embeddings]() <br>&emsp;[3.3 Subword Embeddings]()| &emsp;Topic <br>&emsp;Topic <br>&emsp;Topic <br>&emsp;Topic | &emsp;âœ… <br>&emsp;âœ… <br>&emsp;âœ… <br>&emsp;âœ…|
|[4. Dense vector]()| &emsp;Topic | &emsp;âœ… |
|[5. LLM Learning Types]() <br>&emsp;[5.1 Autoencoding Learning]() <br>&emsp;[5.2 Autoregressive Learning]() <br>&emsp;[5.3 Masked Language Modeling]() <br>&emsp;[5.4 Sequence Learning]()| &emsp;Topic <br>&emsp;Topic <br>&emsp;Topic <br>&emsp;Topic <br>&emsp;Topic | &emsp;âœ… <br>&emsp;âœ… <br>&emsp;âœ… <br>&emsp;âœ… <br>&emsp;âœ…|
|[4. BERT Model]()| &emsp;Topic | &emsp;âœ… |
|[4. GPT Model]()| &emsp;Topic | &emsp;âœ… |
|[4. XLNET Model]()| &emsp;Topic | &emsp;âœ… |

> âœ¨ *Tip: Use consistent naming and categorization to keep your index organized and searchable.*
